# QuantOS

Personal quantitative assistant operating system.

Solves LLM shortcomings:
- inability of LLMs to reliably calculate: solved via **code generation**
- each bit of output adds to latency: solved via using **small fine tuned models**
- high stochasticity: solved via **agentic workflow**

## Inspiration

Modern UIs have **main** part of the content and also **optional** add-ons (in pre-ML world these would be notification counters, some UI affordances, etc).

We envision that **main** content would be generated by agentic workflows with large models while **optional** content would be made by small _fast fine-tuned models_.

## Demo

We demonstrate:
- agentic workflow with large model (Gemini 2.5 Pro) which produces **code** on the fly which ultimately results in chart (quants produce charts:)
- meanwhile medium size model (Gemini Flash 2.5) instructs small models to produce jokes (i.e. optional content)
- small model is a fine tunned Llama which runs on Tenstorrent N300s
- ultimately both chart and joke are shown to the user

## Fine Tunned TT Model running on N300s Koyeb making jokes:

<img width="717" alt="image" src="https://github.com/user-attachments/assets/f42f449b-3d4a-4e2c-bf92-8fad048a1ca4" />

## Agentic Flow output:

**User query:** make some beautiful chart

<img width="847" alt="image" src="https://github.com/user-attachments/assets/8a58ddc0-9ea0-4365-9718-bbfc4c9e2630" />

**User query:** chart showing stock option is pricing

![image](https://github.com/user-attachments/assets/732dac83-da69-4394-bc7e-9587228bbb0c)

**User query:** illustrate law of large numbers for dice rolls

![image](https://github.com/user-attachments/assets/869e7590-f110-436b-bca3-77c0086c6205)

**User query:** gdp vs inflation trends

![image](https://github.com/user-attachments/assets/90bf31f0-544d-4dfc-a5df-c8604f5f2d86)

## Fine tunned model running on Koyeb

<img width="847" alt="image" src="https://github.com/user-attachments/assets/571d4d80-061e-4e8e-a912-f7376bc8b290" />

Fine tuning logs:
```monospace
root@1e8ab32c:/workdir/tenstorrent-examples/tt-models/engines/vllm# docker build -f ../../../Dockerfile.llama1B -t my-llama1b --build-arg HF_MODEL=meta-llama/Llama-3.2-1B-Instruct --secret id=hf_token,env=HF_TOKEN_VAR .
....
root@1e8ab32c:/workdir/tenstorrent-examples/tt-models/engines/vllm# docker push yavol/yv-tt-llama1b:latest
....
acc587849aea: Pushed 
9568f1e3d313: Pushed 
a881565c085a: Pushed 
6872c8af7e00: Pushed 
latest: digest: sha256:c21faf30ecb509a0525783cd7d5e525ad9e8079da6b991e74becd2fb1fd8ec04 size: 11239
root@1e8ab32c:/workdir/tenstorrent-examples/tt-models/engines/vllm# 
```

## Fine Tunning

```sh
$ git clone https://github.com/koyeb/tenstorrent-examples.git

$ pip install huggingface_hub

~/tenstorrent-examples/tt-models$ python utils/scripts/generate_dockerfile_from_tpl.py --model=meta-llama/Llama-3.2-1B-Instruct --dockerfile-template=vllm_template.Dockerfile.jinja --dockerfile-output=Dockerfile.llama1B

# <--- update the Dockerfile.llama1B with fine tunned weights

tenstorrent-examples/tt-models/engines/vllm# docker build -f ../../../Dockerfile.llama1B -t my-llama1b .
```
